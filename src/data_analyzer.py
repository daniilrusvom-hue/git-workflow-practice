Git Init
git init ‚Äî —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–π –ª–æ–∫–∞–ª—å–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –≤ —Ç–µ–∫—É—â–µ–π –ø–∞–ø–∫–µ, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—è —Å–∫—Ä—ã—Ç—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é .git.
Git Config
git config user.name "..." –∏ git config user.email "..." ‚Äî –∑–∞–¥–∞—é—Ç –∏–º—è –∏ email –∞–≤—Ç–æ—Ä–∞ –∫–æ–º–º–∏—Ç–æ–≤ (–º–æ–∂–Ω–æ –≥–ª–æ–±–∞–ª—å–Ω–æ —Å --global).
–†–∞–±–æ—á–∏–π —Ü–∏–∫–ª
‚Ä¢    git status ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ (–∏–∑–º–µ–Ω–µ–Ω—ã, –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –∏ —Ç.–¥.).
‚Ä¢    git add <file> –∏–ª–∏ git add . ‚Äî –¥–æ–±–∞–≤–ª—è–µ—Ç —Ñ–∞–π–ª—ã –≤ –∏–Ω–¥–µ–∫—Å (staging).
‚Ä¢    git commit -m "..." ‚Äî —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–º.
‚Ä¢    git log --oneline --graph ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –∫–æ–º–º–∏—Ç–æ–≤ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π –≤–µ—Ç–æ–∫.
–£–¥–∞–ª—ë–Ω–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
‚Ä¢    git remote add origin <url> ‚Äî –ø—Ä–∏–≤—è–∑—ã–≤–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∫ —É–¥–∞–ª—ë–Ω–Ω–æ–º—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ GitHub).
‚Ä¢    git push -u origin main ‚Äî –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–º–º–∏—Ç—ã –≤ —É–¥–∞–ª—ë–Ω–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤–µ—Ç–∫–∏.
README.md
–§–∞–π–ª —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –ø—Ä–æ–µ–∫—Ç–∞ –≤ Markdown:
‚Ä¢    # –ó–∞–≥–æ–ª–æ–≤–æ–∫, ## –ü–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫
‚Ä¢    - —Å–ø–∏—Å–æ–∫, 1. –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
‚Ä¢    –∫–æ–¥ –∏–ª–∏ –±–ª–æ–∫–∏ –∫–æ–¥–∞ —Å —Ç—Ä–æ–π–Ω—ã–º–∏ –∞–ø–æ—Å—Ç—Ä–æ—Ñ–∞–º–∏.
–í–µ—Ç–≤–ª–µ–Ω–∏–µ –∏ —Å–ª–∏—è–Ω–∏–µ
‚Ä¢    git branch <name> ‚Äî —Å–æ–∑–¥–∞—ë—Ç –≤–µ—Ç–∫—É.
‚Ä¢    git checkout -b <name> ‚Äî —Å–æ–∑–¥–∞—ë—Ç –∏ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ –≤–µ—Ç–∫—É.
‚Ä¢    git merge <branch> ‚Äî –≤–ª–∏–≤–∞–µ—Ç –≤–µ—Ç–∫—É –≤ —Ç–µ–∫—É—â—É—é.
‚Ä¢    Fast-forward ‚Äî –µ—Å–ª–∏ –Ω–µ—Ç —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π, —É–∫–∞–∑–∞—Ç–µ–ª—å –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–º–µ—â–∞–µ—Ç—Å—è.
‚Ä¢    –ù–µ fast-forward ‚Äî —Å–æ–∑–¥–∞—ë—Ç—Å—è –∫–æ–º–º–∏—Ç —Å–ª–∏—è–Ω–∏—è –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π.
–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
–ü—Ä–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–µ —Å–ª–∏—è–Ω–∏—è:
–í—Ä—É—á–Ω—É—é —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª (—É–±–∏—Ä–∞–µ–º –º–∞—Ä–∫–µ—Ä—ã <<<<<<<, =======, >>>>>>>).
git add <file> ‚Äî –ø–æ–º–µ—á–∞–µ–º –∫–æ–Ω—Ñ–ª–∏–∫—Ç —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–º.
git commit ‚Äî –∑–∞–≤–µ—Ä—à–∞–µ–º —Å–ª–∏—è–Ω–∏–µ.
–ü—Ä–æ—Å–º–æ—Ç—Ä –∏—Å—Ç–æ—Ä–∏–∏
git log —Å —Ñ–ª–∞–≥–∞–º–∏:
‚Ä¢    --oneline ‚Äî –∫—Ä–∞—Ç–∫–∏–π –≤–∏–¥.
‚Ä¢    --graph ‚Äî –≥—Ä–∞—Ñ–∏–∫ –≤–µ—Ç–≤–ª–µ–Ω–∏—è.
‚Ä¢    --all ‚Äî –≤—Å–µ –≤–µ—Ç–∫–∏.
‚Ä¢    -p ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ñ–∞–π–ª–∞—Ö.
Gitignore
–§–∞–π–ª .gitignore —Å–æ–¥–µ—Ä–∂–∏—Ç —à–∞–±–ª–æ–Ω—ã —Ñ–∞–π–ª–æ–≤/–ø–∞–ø–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ Git –¥–æ–ª–∂–µ–Ω –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, *.log, node_modules/).
–∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã:
1. –°–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è –≤–µ—Ä—Å–∏–π (VCS)
–°–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è –≤–µ—Ä—Å–∏–π ‚Äî —ç—Ç–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–º–∏. –ö–ª—é—á–µ–≤–∞—è –ø—Ä–æ–±–ª–µ–º–∞, –∫–æ—Ç–æ—Ä—É—é –æ–Ω–∞ —Ä–µ—à–∞–µ—Ç ‚Äî –ø–æ—Ç–µ—Ä—è –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ –ø—Ä–∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π —Ä–∞–±–æ—Ç–µ. –ë–µ–∑ VCS —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å:
‚Ä¢	–í–µ—Ä—Å–∏–æ–Ω–Ω—ã–º —Ö–∞–æ—Å–æ–º ("final_final_version.zip")
‚Ä¢	–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –æ—Ç–∫–∞—Ç–∞ –∫ —Ä–∞–±–æ—á–µ–π –≤–µ—Ä—Å–∏–∏
‚Ä¢	–ö–æ–Ω—Ñ–ª–∏–∫—Ç–∞–º–∏ –ø—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–æ–≤
–†–∞—Å–∫—Ä—ã—Ç—å
message.txt
7 –∫–±
Danielka ‚Äî 19:42
git-workflow-practice
MIT License
Git Workflow Practice Project
üéØ –¶–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞
–û—Å–≤–æ–∏—Ç—å –±–∞–∑–æ–≤—ã–π —Ä–∞–±–æ—á–∏–π —Ü–∏–∫–ª Git: status, add, commit

üìö –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å Git —á–µ—Ä–µ–∑ GitHub –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
–ü—Ä–∞–∫—Ç–∏–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –∫–æ–º–º–∏—Ç–æ–≤
–ò–∑—É—á–µ–Ω–∏–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–µ—Ä—Å–∏—è–º–∏ –ø—Ä–æ–µ–∫—Ç–∞

üèó –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
project/
‚îú‚îÄ‚îÄ README.md # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞
‚îú‚îÄ‚îÄ .gitignore # –ò—Å–∫–ª—é—á–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã
‚îî‚îÄ‚îÄ LICENSE # –õ–∏—Ü–µ–Ω–∑–∏—è MIT

üë®‚Äçüíª –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫
[–î–∞–Ω–∏–∏–ª] - –ì—Ä—É–ø–ø–∞ [–¢9-–ò–ü-24-2]
Danielka ‚Äî 19:55
https://chat.deepseek.com/share/fz3x324pz5gii4zkud
Danielka ‚Äî 20:10
https://chat.deepseek.com/share/zrhose566rb3om5lm5
Danielka ‚Äî 20:41
–ó–∞–¥–∞–Ω–∏–µ 2,3.
–ò–∑—É—á–∏—Ç—å –±–∞–∑–æ–≤—ã–π —Ä–∞–±–æ—á–∏–π —Ü–∏–∫–ª Git: (status, add, commit). –°–æ–∑–¥–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –∫–æ–º–º–∏—Ç–æ–≤, –∏–∑–º–µ–Ω—è—è –∫–æ–¥ —É—á–µ–±–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞, —Å–æ–∑–¥–∞—Ç—å —É–¥–∞–ª–µ–Ω–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–∞ GitHub (GitLab) –∏ —Å–≤—è–∑–∞—Ç—å –µ–≥–æ —Å –ª–æ–∫–∞–ª—å–Ω—ã–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–º (remote, push). –û—Ñ–æ—Ä–º–∏—Ç—å README.md-—Ñ–∞–π–ª –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞.
Boogie
–ë–û–¢
 ‚Äî 20:51
Nothing Playing
No track is currently playing. load a track to get started;)
To disable this feature use /settings
More Features...
Danielka ‚Äî 20:58
feature/new-functionality
Danielka ‚Äî 21:30
src/data_analyzer.py
"""
Data Analysis Module - New feature for Python Git project
Advanced data analysis capabilities using pandas and numpy
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Union
import json

class DataAnalyzer:
    """
    Advanced data analysis tool for statistical operations
    and data visualization preparation
    """
    
    def __init__(self, data_source: str = None):
        self.data_source = data_source
        self.dataset = None
        self.analysis_results = {}
        
    def load_csv_data(self, file_path: str) -> pd.DataFrame:
        """
        Load data from CSV file into pandas DataFrame
        
        Args:
            file_path (str): Path to CSV file
            
        Returns:
            pd.DataFrame: Loaded dataset
        """
        try:
            self.dataset = pd.read_csv(file_path)
            print(f"‚úÖ Successfully loaded data from {file_path}")
            print(f"üìä Dataset shape: {self.dataset.shape}")
            return self.dataset
        except Exception as e:
            print(f"‚ùå Error loading data: {e}")
            return None
    
    def generate_sample_data(self, rows: int = 100) -> pd.DataFrame:
        """
        Generate sample dataset for demonstration
        
        Args:
            rows (int): Number of rows to generate
            
        Returns:
            pd.DataFrame: Generated sample data
        """
        np.random.seed(42)
        
        data = {
            'user_id': range(1, rows + 1),
            'age': np.random.randint(18, 65, rows),
            'salary': np.random.normal(50000, 15000, rows).round(2),
            'department': np.random.choice(['IT', 'HR', 'Finance', 'Marketing'], rows),
            'experience_years': np.random.randint(0, 20, rows),
            'performance_score': np.random.uniform(0.5, 1.0, rows).round(3)
        }
        
        self.dataset = pd.DataFrame(data)
        print(f"üéØ Generated sample dataset with {rows} rows")
        return self.dataset
    
    def basic_statistics(self) -> Dict:
        """
        Calculate basic statistical metrics for numerical columns
        
        Returns:
            Dict: Statistical summary
        """
        if self.dataset is None:
            print("‚ùå No dataset loaded. Generate or load data first.")
            return {}
        
        numerical_cols = self.dataset.select_dtypes(include=[np.number]).columns
        
        stats = {}
        for col in numerical_cols:
            stats[col] = {
                'mean': float(self.dataset[col].mean()),
                'median': float(self.dataset[col].median()),
                'std': float(self.dataset[col].std()),
                'min': float(self.dataset[col].min()),
                'max': float(self.dataset[col].max()),
                'count': int(self.dataset[col].count())
            }
        
        self.analysis_results['basic_stats'] = stats
        return stats
    
    def group_analysis(self, group_by: str, target_column: str) -> Dict:
        """
        Analyze data grouped by specific column
        
        Args:
            group_by (str): Column to group by
            target_column (str): Column to analyze
            
... (–æ—Å—Ç–∞–ª–æ—Å—å: 126 —Å—Ç—Ä–æ–∫)
–°–≤–µ—Ä–Ω—É—Ç—å
message.txt
8 –∫–±
Ôªø
"""
Data Analysis Module - New feature for Python Git project
Advanced data analysis capabilities using pandas and numpy
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Union
import json

class DataAnalyzer:
    """
    Advanced data analysis tool for statistical operations
    and data visualization preparation
    """
    
    def __init__(self, data_source: str = None):
        self.data_source = data_source
        self.dataset = None
        self.analysis_results = {}
        
    def load_csv_data(self, file_path: str) -> pd.DataFrame:
        """
        Load data from CSV file into pandas DataFrame
        
        Args:
            file_path (str): Path to CSV file
            
        Returns:
            pd.DataFrame: Loaded dataset
        """
        try:
            self.dataset = pd.read_csv(file_path)
            print(f"‚úÖ Successfully loaded data from {file_path}")
            print(f"üìä Dataset shape: {self.dataset.shape}")
            return self.dataset
        except Exception as e:
            print(f"‚ùå Error loading data: {e}")
            return None
    
    def generate_sample_data(self, rows: int = 100) -> pd.DataFrame:
        """
        Generate sample dataset for demonstration
        
        Args:
            rows (int): Number of rows to generate
            
        Returns:
            pd.DataFrame: Generated sample data
        """
        np.random.seed(42)
        
        data = {
            'user_id': range(1, rows + 1),
            'age': np.random.randint(18, 65, rows),
            'salary': np.random.normal(50000, 15000, rows).round(2),
            'department': np.random.choice(['IT', 'HR', 'Finance', 'Marketing'], rows),
            'experience_years': np.random.randint(0, 20, rows),
            'performance_score': np.random.uniform(0.5, 1.0, rows).round(3)
        }
        
        self.dataset = pd.DataFrame(data)
        print(f"üéØ Generated sample dataset with {rows} rows")
        return self.dataset
    
    def basic_statistics(self) -> Dict:
        """
        Calculate basic statistical metrics for numerical columns
        
        Returns:
            Dict: Statistical summary
        """
        if self.dataset is None:
            print("‚ùå No dataset loaded. Generate or load data first.")
            return {}
        
        numerical_cols = self.dataset.select_dtypes(include=[np.number]).columns
        
        stats = {}
        for col in numerical_cols:
            stats[col] = {
                'mean': float(self.dataset[col].mean()),
                'median': float(self.dataset[col].median()),
                'std': float(self.dataset[col].std()),
                'min': float(self.dataset[col].min()),
                'max': float(self.dataset[col].max()),
                'count': int(self.dataset[col].count())
            }
        
        self.analysis_results['basic_stats'] = stats
        return stats
    
    def group_analysis(self, group_by: str, target_column: str) -> Dict:
        """
        Analyze data grouped by specific column
        
        Args:
            group_by (str): Column to group by
            target_column (str): Column to analyze
            
        Returns:
            Dict: Grouped analysis results
        """
        if self.dataset is None:
            print("‚ùå No dataset loaded.")
            return {}
        
        if group_by not in self.dataset.columns or target_column not in self.dataset.columns:
            print("‚ùå Specified columns not found in dataset.")
            return {}
        
        grouped = self.dataset.groupby(group_by)[target_column].agg([
            'count', 'mean', 'std', 'min', 'max'
        ]).round(3)
        
        result = grouped.to_dict()
        self.analysis_results[f'grouped_{group_by}'] = result
        
        print(f"üìà Grouped analysis by '{group_by}':")
        print(grouped)
        
        return result
    
    def detect_outliers(self, column: str, method: str = 'iqr') -> List[int]:
        """
        Detect outliers in specified column
        
        Args:
            column (str): Column to analyze
            method (str): Method for outlier detection ('iqr' or 'zscore')
            
        Returns:
            List[int]: Indices of outlier rows
        """
        if self.dataset is None or column not in self.dataset.columns:
            return []
        
        data = self.dataset[column]
        
        if method == 'iqr':
            Q1 = data.quantile(0.25)
            Q3 = data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = self.dataset[(data < lower_bound) | (data > upper_bound)].index.tolist()
        
        elif method == 'zscore':
            z_scores = np.abs((data - data.mean()) / data.std())
            outliers = self.dataset[z_scores > 3].index.tolist()
        
        print(f"üîç Found {len(outliers)} outliers in '{column}' using {method} method")
        self.analysis_results['outliers'] = outliers
        
        return outliers
    
    def export_results(self, file_path: str = 'analysis_results.json'):
        """
        Export analysis results to JSON file
        
        Args:
            file_path (str): Path for output file
        """
        if not self.analysis_results:
            print("‚ùå No analysis results to export.")
            return
        
        try:
            with open(file_path, 'w') as f:
                # Convert numpy types to Python native types for JSON serialization
                def convert_types(obj):
                    if isinstance(obj, (np.integer, np.floating)):
                        return float(obj)
                    elif isinstance(obj, np.ndarray):
                        return obj.tolist()
                    elif isinstance(obj, dict):
                        return {k: convert_types(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [convert_types(item) for item in obj]
                    return obj
                
                json.dump(convert_types(self.analysis_results), f, indent=2)
            print(f"üíæ Analysis results exported to {file_path}")
        except Exception as e:
            print(f"‚ùå Error exporting results: {e}")


def demo_data_analysis():
    """
    Demonstration function for the DataAnalyzer class
    """
    print("üöÄ Data Analysis Module Demonstration")
    print("=" * 50)
    
    # Initialize analyzer
    analyzer = DataAnalyzer()
    
    # Generate sample data
    print("\n1. Generating sample dataset...")
    dataset = analyzer.generate_sample_data(150)
    print(f"   Columns: {list(dataset.columns)}")
    
    # Basic statistics
    print("\n2. Calculating basic statistics...")
    stats = analyzer.basic_statistics()
    for col, metrics in stats.items():
        print(f"   {col}: mean={metrics['mean']:.2f}, std={metrics['std']:.2f}")
    
    # Group analysis
    print("\n3. Performing group analysis...")
    analyzer.group_analysis('department', 'salary')
    
    # Outlier detection
    print("\n4. Detecting outliers...")
    outliers = analyzer.detect_outliers('salary')
    print(f"   Found {len(outliers)} outliers in salary data")
    
    # Export results
    print("\n5. Exporting results...")
    analyzer.export_results('demo_analysis.json')
    
    print("\nüéâ Data analysis demonstration completed!")


if __name__ == "__main__":
    demo_data_analysis()
